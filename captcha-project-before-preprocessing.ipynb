{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10749816,"sourceType":"datasetVersion","datasetId":6667049},{"sourceId":10755412,"sourceType":"datasetVersion","datasetId":6670928}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install openai-whisper","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T17:01:51.509319Z","iopub.execute_input":"2025-02-17T17:01:51.509783Z","iopub.status.idle":"2025-02-17T17:02:16.724031Z","shell.execute_reply.started":"2025-02-17T17:01:51.509725Z","shell.execute_reply":"2025-02-17T17:02:16.722852Z"}},"outputs":[{"name":"stdout","text":"Collecting openai-whisper\n  Downloading openai-whisper-20240930.tar.gz (800 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.60.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.26.4)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.5.1+cu121)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.67.1)\nRequirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.5.0)\nRequirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.8.0)\nCollecting triton>=2.0.0 (from openai-whisper)\n  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.43.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->openai-whisper) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->openai-whisper) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->openai-whisper) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->openai-whisper) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->openai-whisper) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->openai-whisper) (2.4.1)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.32.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->openai-whisper) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->openai-whisper) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->openai-whisper) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->openai-whisper) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->openai-whisper) (2024.2.0)\nDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: openai-whisper\n  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803373 sha256=1cafcba79223cde9f8be96659fee4138af1c6828d5f71a45a9887d148e55a003\n  Stored in directory: /root/.cache/pip/wheels/dd/4a/1f/d1c4bf3b9133c8168fe617ed979cab7b14fe381d059ffb9d83\nSuccessfully built openai-whisper\nInstalling collected packages: triton, openai-whisper\nSuccessfully installed openai-whisper-20240930 triton-3.2.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install jiwer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T17:02:23.287659Z","iopub.execute_input":"2025-02-17T17:02:23.288002Z","iopub.status.idle":"2025-02-17T17:02:31.017349Z","shell.execute_reply.started":"2025-02-17T17:02:23.287973Z","shell.execute_reply":"2025-02-17T17:02:31.016075Z"}},"outputs":[{"name":"stdout","text":"Collecting jiwer\n  Downloading jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\nCollecting click>=8.1.8 (from jiwer)\n  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\nCollecting rapidfuzz>=3.9.7 (from jiwer)\n  Downloading rapidfuzz-3.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nDownloading jiwer-3.1.0-py3-none-any.whl (22 kB)\nDownloading click-8.1.8-py3-none-any.whl (98 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading rapidfuzz-3.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, click, jiwer\n  Attempting uninstall: click\n    Found existing installation: click 8.1.7\n    Uninstalling click-8.1.7:\n      Successfully uninstalled click-8.1.7\nSuccessfully installed click-8.1.8 jiwer-3.1.0 rapidfuzz-3.12.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import whisper\nimport os\nimport random\nimport torch\nimport librosa\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pydub import AudioSegment\nfrom easyocr import Reader\nfrom transformers import WhisperProcessor\nfrom torch.utils.data import DataLoader\nfrom torch import nn\nimport re\nimport soundfile as sf\nfrom jiwer import wer, cer  \nfrom tqdm import tqdm ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T17:02:34.404765Z","iopub.execute_input":"2025-02-17T17:02:34.405147Z","iopub.status.idle":"2025-02-17T17:02:48.982573Z","shell.execute_reply.started":"2025-02-17T17:02:34.405112Z","shell.execute_reply":"2025-02-17T17:02:48.981484Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Dataset Path","metadata":{}},{"cell_type":"code","source":"# Directories\naudio_dir = '/kaggle/input/captcha-dataset/captchas/audio'\nimages_dir = '/kaggle/input/captcha-dataset/captchas/images'\n\naudio_files = [f for f in os.listdir(audio_dir) if f.endswith('.wav')]\nimage_files = [f for f in os.listdir(images_dir) if f.endswith('.png')]\n\nprint(f\"Total audio files: {len(audio_files)}\")\nprint(f\"Total image files: {len(image_files)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T17:05:55.568985Z","iopub.execute_input":"2025-02-17T17:05:55.569602Z","iopub.status.idle":"2025-02-17T17:05:56.467687Z","shell.execute_reply.started":"2025-02-17T17:05:55.569567Z","shell.execute_reply":"2025-02-17T17:05:56.466397Z"}},"outputs":[{"name":"stdout","text":"Total audio files: 10000\nTotal image files: 10000\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Implementation of Whisper AI(small) and OCR to generate text for 10 random samples","metadata":{}},{"cell_type":"code","source":"audio_dir = '/kaggle/input/captcha-dataset/captchas/audio'\nimage_dir = '/kaggle/input/captcha-dataset/captchas/images'\n\n# Initialize EasyOCR and Whisper\nocr_reader = Reader(['en'])  # EasyOCR setup\nwhisper_model = whisper.load_model(\"base\")  # Whisper model\n\n# Dictionary to convert number words to digits\nnumber_map = {\n    \"zero\": \"0\", \"one\": \"1\", \"two\": \"2\", \"three\": \"3\", \"four\": \"4\",\n    \"five\": \"5\", \"six\": \"6\", \"seven\": \"7\", \"eight\": \"8\", \"nine\": \"9\"\n}\n\n# Dictionary for common misheard words\nmisheard_map = {\n    \"mall\": \"small\", \"moll\": \"small\", \"capitun\": \"capital\", \"capitan\": \"capital\",\n    \"apital\": \"capital\", \"capitole\": \"capital\", \"zimro\": \"0\", \"smaller\": \"a\"\n}\n\n# Step 1: Replace number words with digits\ndef replace_number_words(text):\n    for word, digit in number_map.items():\n        text = re.sub(rf\"\\b{word}\\b\", digit, text, flags=re.IGNORECASE)\n    return text\n\n# Step 2: Fix common misheard words\ndef fix_misheard_words(text):\n    for wrong, correct in misheard_map.items():\n        text = re.sub(rf\"\\b{wrong}\\b\", correct, text, flags=re.IGNORECASE)\n    return text\n\n# Step 3: Replace \"for\" with \"4\"\ndef replace_for_with_4(text):\n    text = re.sub(r\"\\bfor\\b\", \"4\", text, flags=re.IGNORECASE)\n    return text\n\n# Step 4: Process capitalization and handle \"edge\" replacement\ndef process_capitalization(text):\n    matches = re.findall(r\"(capital|small) (\\w+)\", text, re.IGNORECASE)\n    cleaned_text = \"\"\n\n    i = 0\n    while i < len(text):\n        match_found = False\n        \n        for match in matches:\n            marker, letter = match\n            marker_index = text.lower().find(f\"{marker.lower()} {letter.lower()}\")\n            \n            if marker_index == i:\n                if marker.lower() == \"capital\":\n                    cleaned_text += letter.upper()\n                else:\n                    cleaned_text += letter.lower()\n                i += len(marker) + 2\n                match_found = True\n                break\n\n        if not match_found:\n            cleaned_text += text[i]\n            i += 1\n\n    # Handle \"edge\" replacement based on the previous marker\n    cleaned_text = re.sub(r\"\\bsmall\\s+edge\\b\", \"h\", cleaned_text, flags=re.IGNORECASE)\n    cleaned_text = re.sub(r\"\\bcapital\\s+edge\\b\", \"H\", cleaned_text, flags=re.IGNORECASE)\n\n    # Remove remaining \"capital\" or \"small\" words\n    cleaned_text = re.sub(r\"\\b(capital|small)\\b\", \"\", cleaned_text, flags=re.IGNORECASE)\n    cleaned_text = re.sub(r\"[^a-zA-Z0-9]\", \"\", cleaned_text)  # Keep only A-Z, a-z, 0-9\n    return cleaned_text\n\n# Function to get Whisper transcription\ndef get_whisper_transcription(audio_file):\n    text = \"\"\n    \n    # Try Whisper transcription up to 3 times if length is < 6\n    for _ in range(3):\n        result = whisper_model.transcribe(audio_file)\n        text = result[\"text\"]\n        text = replace_number_words(text)  # Replace number words with digits\n        text = fix_misheard_words(text)  # Fix misheard words\n        text = replace_for_with_4(text)  # Replace \"for\" with \"4\"\n        text = process_capitalization(text)  # Process capitalization & \"edge\"\n        \n        if len(text) >= 6:  # Ensure minimum length of 6\n            break\n\n    return text[:6]  # Ensure the output is exactly 6 characters\n\n# Function to get text from image using EasyOCR\ndef get_image_text(image_file):\n    ocr_result = ocr_reader.readtext(image_file)\n    text = ''.join([res[1] for res in ocr_result])\n    return text\n\ndef process_random_samples(audio_dir, image_dir, num_samples=10):\n    audio_files = random.sample(os.listdir(audio_dir), num_samples)\n    \n    for audio_file in audio_files:\n        audio_path = os.path.join(audio_dir, audio_file)\n        image_file = audio_file.replace('.wav', '.png')  # Assuming same filename for audio and image\n        image_path = os.path.join(image_dir, image_file)\n\n        whisper_text = get_whisper_transcription(audio_path)\n        ground_truth_text = get_image_text(image_path)\n\n        print(f\"Audio file: {audio_file}\")\n        print(f\"Whisper AI Transcription: {whisper_text}\")\n        print(f\"Ground Truth (OCR): {ground_truth_text}\")\n        print(\"=\"*50)\n\n# Run the function to process random samples\nprocess_random_samples(audio_dir, image_dir, num_samples=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T18:31:10.209506Z","iopub.execute_input":"2025-02-16T18:31:10.209866Z","iopub.status.idle":"2025-02-16T18:31:18.844813Z","shell.execute_reply.started":"2025-02-16T18:31:10.209840Z","shell.execute_reply":"2025-02-16T18:31:18.843819Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(fp, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Audio file: captcha_2800.wav\nWhisper AI Transcription: P0h1kQ\nGround Truth (OCR): PohzkQ\n==================================================\nAudio file: captcha_9383.wav\nWhisper AI Transcription: pK2Udc\nGround Truth (OCR): pKZUdc\n==================================================\nAudio file: captcha_0862.wav\nWhisper AI Transcription: mVv2N\nGround Truth (OCR): mBZv2N\n==================================================\nAudio file: captcha_2692.wav\nWhisper AI Transcription: F4aE13\nGround Truth (OCR): FaaE13\n==================================================\nAudio file: captcha_5985.wav\nWhisper AI Transcription: ASVKVA\nGround Truth (OCR): AsVKBE\n==================================================\nAudio file: captcha_8862.wav\nWhisper AI Transcription: apGv5v\nGround Truth (OCR): epGvsb\n==================================================\nAudio file: captcha_2196.wav\nWhisper AI Transcription: LHlzs7\nGround Truth (OCR): LHlzs7\n==================================================\nAudio file: captcha_0318.wav\nWhisper AI Transcription: PHW1Qb\nGround Truth (OCR): PHWIQb\n==================================================\nAudio file: captcha_3554.wav\nWhisper AI Transcription: wi8ciu\nGround Truth (OCR): wi8ciu\n==================================================\nAudio file: captcha_0087.wav\nWhisper AI Transcription: auIGL0\nGround Truth (OCR): aulGLO\n==================================================\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# Whisper AI(small) Matching Accuracy Scores","metadata":{}},{"cell_type":"code","source":"audio_dir = '/kaggle/input/captcha-dataset/captchas/audio'\nimage_dir = '/kaggle/input/captcha-dataset/captchas/images'\n\nocr_reader = Reader(['en'])  # EasyOCR setup\nwhisper_model = whisper.load_model(\"base\")  # Whisper model\n\n# Dictionary to convert number words to digits\nnumber_map = {\n    \"zero\": \"0\", \"one\": \"1\", \"two\": \"2\", \"three\": \"3\", \"four\": \"4\",\n    \"five\": \"5\", \"six\": \"6\", \"seven\": \"7\", \"eight\": \"8\", \"nine\": \"9\"\n}\n\n# Dictionary for common misheard words\nmisheard_map = {\n    \"mall\": \"small\", \"moll\": \"small\", \"capitun\": \"capital\", \"capitan\": \"capital\",\n    \"apital\": \"capital\", \"capitole\": \"capital\", \"zimro\": \"0\", \"smaller\": \"a\"\n}\n\n# Step 1: Replace number words with digits\ndef replace_number_words(text):\n    for word, digit in number_map.items():\n        text = re.sub(rf\"\\b{word}\\b\", digit, text, flags=re.IGNORECASE)\n    return text\n\n# Step 2: Fix common misheard words\ndef fix_misheard_words(text):\n    for wrong, correct in misheard_map.items():\n        text = re.sub(rf\"\\b{wrong}\\b\", correct, text, flags=re.IGNORECASE)\n    return text\n\n# Step 3: Replace \"for\" with \"4\"\ndef replace_for_with_4(text):\n    text = re.sub(r\"\\bfor\\b\", \"4\", text, flags=re.IGNORECASE)\n    return text\n\n# Step 4: Process capitalization and handle \"edge\" replacement\ndef process_capitalization(text):\n    matches = re.findall(r\"(capital|small) (\\w+)\", text, re.IGNORECASE)\n    cleaned_text = \"\"\n\n    i = 0\n    while i < len(text):\n        match_found = False\n        \n        for match in matches:\n            marker, letter = match\n            marker_index = text.lower().find(f\"{marker.lower()} {letter.lower()}\")\n            \n            if marker_index == i:\n                if marker.lower() == \"capital\":\n                    cleaned_text += letter.upper()\n                else:\n                    cleaned_text += letter.lower()\n                i += len(marker) + 2\n                match_found = True\n                break\n\n        if not match_found:\n            cleaned_text += text[i]\n            i += 1\n\n    # Handle \"edge\" replacement based on the previous marker\n    cleaned_text = re.sub(r\"\\bsmall\\s+edge\\b\", \"h\", cleaned_text, flags=re.IGNORECASE)\n    cleaned_text = re.sub(r\"\\bcapital\\s+edge\\b\", \"H\", cleaned_text, flags=re.IGNORECASE)\n\n    # Remove remaining \"capital\" or \"small\" words\n    cleaned_text = re.sub(r\"\\b(capital|small)\\b\", \"\", cleaned_text, flags=re.IGNORECASE)\n    cleaned_text = re.sub(r\"[^a-zA-Z0-9]\", \"\", cleaned_text)  # Keep only A-Z, a-z, 0-9\n    return cleaned_text\n\n# Function to get Whisper transcription\ndef get_whisper_transcription(audio_file):\n    text = \"\"\n    \n    # Try Whisper transcription up to 3 times if length is < 6\n    for _ in range(3):\n        result = whisper_model.transcribe(audio_file)\n        text = result[\"text\"]\n        text = replace_number_words(text)  # Replace number words with digits\n        text = fix_misheard_words(text)  # Fix misheard words\n        text = replace_for_with_4(text)  # Replace \"for\" with \"4\"\n        text = process_capitalization(text)  # Process capitalization & \"edge\"\n        \n        if len(text) >= 6:  # Ensure minimum length of 6\n            break\n\n    return text[:6]  # Ensure the output is exactly 6 characters\n\ndef get_image_text(image_file):\n    ocr_result = ocr_reader.readtext(image_file)\n    text = ''.join([res[1] for res in ocr_result])\n    return text\n\ndef process_all_samples(audio_dir, image_dir):\n    all_files = os.listdir(audio_dir)\n    total_files = len(all_files)\n    cer_scores = []\n    \n    for i, audio_file in enumerate(tqdm(all_files, desc=\"Processing\")):\n        audio_path = os.path.join(audio_dir, audio_file)\n        image_file = audio_file.replace('.wav', '.png')  # Assuming matching names\n        image_path = os.path.join(image_dir, image_file)\n\n        whisper_text = get_whisper_transcription(audio_path)\n\n        ground_truth_text = get_image_text(image_path)\n\n        if not ground_truth_text or not whisper_text:\n            continue \n\n        # Compute CER (Character Error Rate)\n        error_rate = cer(ground_truth_text, whisper_text)  # CER calculation\n        accuracy = max(0, 100 - (error_rate * 100))  \n        cer_scores.append(accuracy)\n\n        # Print progress every 500 samples\n        if (i + 1) % 500 == 0:\n            avg_accuracy = sum(cer_scores) / len(cer_scores)\n            print(f\"\\nProgress: {i+1}/{total_files} samples processed. Current Accuracy: {avg_accuracy:.2f}%\\n\")\n\n    # Final average accuracy\n    avg_accuracy = sum(cer_scores) / len(cer_scores) if cer_scores else 0\n    print(f\"\\nFinal Accuracy after processing {total_files} samples: {avg_accuracy:.2f}%\\n\")\n\n# Run the function to process all samples\nprocess_all_samples(audio_dir, image_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T19:13:15.385821Z","iopub.execute_input":"2025-02-16T19:13:15.386183Z","iopub.status.idle":"2025-02-16T20:30:24.396829Z","shell.execute_reply.started":"2025-02-16T19:13:15.386155Z","shell.execute_reply":"2025-02-16T20:30:24.395983Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(fp, map_location=device)\nProcessing:  10%|█         | 1000/10000 [07:28<55:47,  2.69it/s] ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 1000/10000 samples processed. Current Accuracy: 72.88%\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  15%|█▌        | 1500/10000 [11:10<57:26,  2.47it/s]  ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 1500/10000 samples processed. Current Accuracy: 73.08%\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  20%|██        | 2000/10000 [15:20<56:21,  2.37it/s]   ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 2000/10000 samples processed. Current Accuracy: 72.77%\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  25%|██▌       | 2500/10000 [19:17<49:48,  2.51it/s]  ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 2500/10000 samples processed. Current Accuracy: 73.43%\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  30%|███       | 3000/10000 [23:24<46:10,  2.53it/s]  ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 3000/10000 samples processed. Current Accuracy: 73.33%\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  35%|███▌      | 3500/10000 [27:19<1:15:09,  1.44it/s]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 3500/10000 samples processed. Current Accuracy: 73.29%\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  40%|████      | 4000/10000 [31:07<47:06,  2.12it/s]  ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 4000/10000 samples processed. Current Accuracy: 73.37%\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  45%|████▌     | 4500/10000 [34:53<54:42,  1.68it/s]  ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 4500/10000 samples processed. Current Accuracy: 73.20%\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  50%|█████     | 5000/10000 [38:43<32:36,  2.56it/s]  ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 5000/10000 samples processed. Current Accuracy: 73.24%\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  55%|█████▌    | 5500/10000 [42:38<3:39:31,  2.93s/it]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 5500/10000 samples processed. Current Accuracy: 73.14%\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  60%|██████    | 6000/10000 [46:33<26:30,  2.51it/s]  ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 6000/10000 samples processed. Current Accuracy: 73.11%\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  65%|██████▌   | 6500/10000 [50:21<29:47,  1.96it/s]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 6500/10000 samples processed. Current Accuracy: 73.12%\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  70%|███████   | 7000/10000 [54:10<19:20,  2.59it/s]  ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 7000/10000 samples processed. Current Accuracy: 73.26%\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  75%|███████▌  | 7500/10000 [57:57<16:19,  2.55it/s]  ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 7500/10000 samples processed. Current Accuracy: 73.26%\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  80%|████████  | 8000/10000 [1:01:33<14:55,  2.23it/s]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 8000/10000 samples processed. Current Accuracy: 73.17%\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  85%|████████▌ | 8500/10000 [1:05:35<17:43,  1.41it/s]  ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 8500/10000 samples processed. Current Accuracy: 73.01%\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  90%|█████████ | 9000/10000 [1:09:19<07:59,  2.09it/s]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 9000/10000 samples processed. Current Accuracy: 72.99%\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  95%|█████████▌| 9500/10000 [1:13:19<03:09,  2.64it/s]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 9500/10000 samples processed. Current Accuracy: 72.98%\n\n","output_type":"stream"},{"name":"stderr","text":"Processing: 100%|██████████| 10000/10000 [1:17:04<00:00,  2.16it/s]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 10000/10000 samples processed. Current Accuracy: 73.07%\n\n\nFinal Accuracy after processing 10000 samples: 73.07%\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"# Whisper AI(small) - No. of texts matched","metadata":{}},{"cell_type":"code","source":"audio_dir = '/kaggle/input/captcha-dataset/captchas/audio'\nimage_dir = '/kaggle/input/captcha-dataset/captchas/images'\n\nocr_reader = Reader(['en'])  # EasyOCR setup\nwhisper_model = whisper.load_model(\"base\")  # Whisper model\n\n# Dictionary to convert number words to digits\nnumber_map = {\n    \"zero\": \"0\", \"one\": \"1\", \"two\": \"2\", \"three\": \"3\", \"four\": \"4\",\n    \"five\": \"5\", \"six\": \"6\", \"seven\": \"7\", \"eight\": \"8\", \"nine\": \"9\"\n}\n\n# Dictionary for common misheard words\nmisheard_map = {\n    \"mall\": \"small\", \"moll\": \"small\", \"capitun\": \"capital\", \"capitan\": \"capital\",\n    \"apital\": \"capital\", \"capitole\": \"capital\", \"zimro\": \"0\", \"smaller\": \"a\"\n}\n\n# Step 1: Replace number words with digits\ndef replace_number_words(text):\n    for word, digit in number_map.items():\n        text = re.sub(rf\"\\b{word}\\b\", digit, text, flags=re.IGNORECASE)\n    return text\n\n# Step 2: Fix common misheard words\ndef fix_misheard_words(text):\n    for wrong, correct in misheard_map.items():\n        text = re.sub(rf\"\\b{wrong}\\b\", correct, text, flags=re.IGNORECASE)\n    return text\n\n# Step 3: Replace \"for\" with \"4\"\ndef replace_for_with_4(text):\n    text = re.sub(r\"\\bfor\\b\", \"4\", text, flags=re.IGNORECASE)\n    return text\n\n# Step 4: Process capitalization and handle \"edge\" replacement\ndef process_capitalization(text):\n    matches = re.findall(r\"(capital|small) (\\w+)\", text, re.IGNORECASE)\n    cleaned_text = \"\"\n\n    i = 0\n    while i < len(text):\n        match_found = False\n        \n        for match in matches:\n            marker, letter = match\n            marker_index = text.lower().find(f\"{marker.lower()} {letter.lower()}\")\n            \n            if marker_index == i:\n                if marker.lower() == \"capital\":\n                    cleaned_text += letter.upper()\n                else:\n                    cleaned_text += letter.lower()\n                i += len(marker) + 2\n                match_found = True\n                break\n\n        if not match_found:\n            cleaned_text += text[i]\n            i += 1\n\n    # Handle \"edge\" replacement based on the previous marker\n    cleaned_text = re.sub(r\"\\bsmall\\s+edge\\b\", \"h\", cleaned_text, flags=re.IGNORECASE)\n    cleaned_text = re.sub(r\"\\bcapital\\s+edge\\b\", \"H\", cleaned_text, flags=re.IGNORECASE)\n\n    # Remove remaining \"capital\" or \"small\" words\n    cleaned_text = re.sub(r\"\\b(capital|small)\\b\", \"\", cleaned_text, flags=re.IGNORECASE)\n    cleaned_text = re.sub(r\"[^a-zA-Z0-9]\", \"\", cleaned_text)  # Keep only A-Z, a-z, 0-9\n    return cleaned_text\n\n# Function to get Whisper transcription\ndef get_whisper_transcription(audio_file):\n    text = \"\"\n    \n    # Try Whisper transcription up to 3 times if length is < 6\n    for _ in range(3):\n        result = whisper_model.transcribe(audio_file)\n        text = result[\"text\"]\n        text = replace_number_words(text)  # Replace number words with digits\n        text = fix_misheard_words(text)  # Fix misheard words\n        text = replace_for_with_4(text)  # Replace \"for\" with \"4\"\n        text = process_capitalization(text)  # Process capitalization & \"edge\"\n        \n        if len(text) >= 6:  # Ensure minimum length of 6\n            break\n\n    return text[:6]  # Ensure the output is exactly 6 characters\n\ndef get_image_text(image_file):\n    ocr_result = ocr_reader.readtext(image_file)\n    text = ''.join([res[1] for res in ocr_result])\n    return text\n\ndef process_all_samples(audio_dir, image_dir):\n    all_files = os.listdir(audio_dir)\n    total_files = len(all_files)\n    exact_match_count = 0  # Counter for exact matches\n    \n    for i, audio_file in enumerate(tqdm(all_files, desc=\"Processing\")):\n        audio_path = os.path.join(audio_dir, audio_file)\n        image_file = audio_file.replace('.wav', '.png')  # Assuming matching names\n        image_path = os.path.join(image_dir, image_file)\n\n        # Get transcription from Whisper\n        whisper_text = get_whisper_transcription(audio_path)\n        \n        # Get ground truth text from OCR\n        ground_truth_text = get_image_text(image_path)\n\n        if not ground_truth_text or not whisper_text:\n            continue  \n\n        if whisper_text == ground_truth_text:\n            exact_match_count += 1\n\n        if (i + 1) % 500 == 0:\n            print(f\"\\nProgress: {i+1}/{total_files} samples processed. Exact Matches: {exact_match_count}\\n\")\n\n    # Final count of exact matches\n    print(f\"\\nFinal Exact Matches after processing {total_files} samples: {exact_match_count}\\n\")\n\n# Run the function to process all samples\nprocess_all_samples(audio_dir, image_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T22:03:16.008617Z","iopub.execute_input":"2025-02-16T22:03:16.008994Z","iopub.status.idle":"2025-02-16T23:17:40.451378Z","shell.execute_reply.started":"2025-02-16T22:03:16.008966Z","shell.execute_reply":"2025-02-16T23:17:40.450648Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(fp, map_location=device)\nProcessing:  10%|█         | 1000/10000 [07:26<55:55,  2.68it/s] ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 1000/10000 samples processed. Exact Matches: 248\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  15%|█▌        | 1500/10000 [11:07<59:14,  2.39it/s]  ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 1500/10000 samples processed. Exact Matches: 369\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  20%|██        | 2000/10000 [14:58<55:49,  2.39it/s]  ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 2000/10000 samples processed. Exact Matches: 480\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  25%|██▌       | 2500/10000 [18:43<47:07,  2.65it/s]  ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 2500/10000 samples processed. Exact Matches: 625\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  30%|███       | 3000/10000 [22:40<43:22,  2.69it/s]  ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 3000/10000 samples processed. Exact Matches: 745\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  35%|███▌      | 3500/10000 [26:24<1:02:44,  1.73it/s]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 3500/10000 samples processed. Exact Matches: 879\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  40%|████      | 4000/10000 [30:00<44:46,  2.23it/s]  ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 4000/10000 samples processed. Exact Matches: 1017\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  45%|████▌     | 4500/10000 [33:40<52:52,  1.73it/s]  ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 4500/10000 samples processed. Exact Matches: 1143\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  50%|█████     | 5000/10000 [37:22<30:57,  2.69it/s]  ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 5000/10000 samples processed. Exact Matches: 1269\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  55%|█████▌    | 5500/10000 [41:12<3:39:59,  2.93s/it]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 5500/10000 samples processed. Exact Matches: 1391\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  60%|██████    | 6000/10000 [45:00<24:44,  2.70it/s]  ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 6000/10000 samples processed. Exact Matches: 1512\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  65%|██████▌   | 6500/10000 [48:38<28:41,  2.03it/s]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 6500/10000 samples processed. Exact Matches: 1637\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  70%|███████   | 7000/10000 [52:19<18:23,  2.72it/s]  ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 7000/10000 samples processed. Exact Matches: 1770\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  75%|███████▌  | 7500/10000 [55:59<15:38,  2.66it/s]  ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 7500/10000 samples processed. Exact Matches: 1900\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  80%|████████  | 8000/10000 [59:30<14:18,  2.33it/s]  ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 8000/10000 samples processed. Exact Matches: 2032\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  85%|████████▌ | 8500/10000 [1:03:26<17:03,  1.47it/s]  ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 8500/10000 samples processed. Exact Matches: 2147\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  90%|█████████ | 9000/10000 [1:06:59<07:28,  2.23it/s]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 9000/10000 samples processed. Exact Matches: 2278\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  95%|█████████▌| 9500/10000 [1:10:47<03:00,  2.78it/s]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 9500/10000 samples processed. Exact Matches: 2401\n\n","output_type":"stream"},{"name":"stderr","text":"Processing: 100%|██████████| 10000/10000 [1:14:20<00:00,  2.24it/s]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 10000/10000 samples processed. Exact Matches: 2542\n\n\nFinal Exact Matches after processing 10000 samples: 2542\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"# Implementation of Whisper AI(medium) and OCR to generate text for 10 random samples","metadata":{}},{"cell_type":"code","source":"audio_dir = '/kaggle/input/captcha-dataset/captchas/audio'\nimage_dir = '/kaggle/input/captcha-dataset/captchas/images'\n\nocr_reader = Reader(['en'])  # EasyOCR setup\nwhisper_model = whisper.load_model(\"medium\")  # Whisper model\n\n# Dictionary to convert number words to digits\nnumber_map = {\n    \"zero\": \"0\", \"one\": \"1\", \"two\": \"2\", \"three\": \"3\", \"four\": \"4\",\n    \"five\": \"5\", \"six\": \"6\", \"seven\": \"7\", \"eight\": \"8\", \"nine\": \"9\"\n}\n\n# Dictionary for common misheard words\nmisheard_map = {\n    \"mall\": \"small\", \"moll\": \"small\", \"capitun\": \"capital\", \"capitan\": \"capital\",\n    \"apital\": \"capital\", \"capitole\": \"capital\", \"zimro\": \"0\", \"smaller\": \"a\"\n}\n\n# Step 1: Replace number words with digits\ndef replace_number_words(text):\n    for word, digit in number_map.items():\n        text = re.sub(rf\"\\b{word}\\b\", digit, text, flags=re.IGNORECASE)\n    return text\n\n# Step 2: Fix common misheard words\ndef fix_misheard_words(text):\n    for wrong, correct in misheard_map.items():\n        text = re.sub(rf\"\\b{wrong}\\b\", correct, text, flags=re.IGNORECASE)\n    return text\n\n# Step 3: Replace \"for\" with \"4\"\ndef replace_for_with_4(text):\n    text = re.sub(r\"\\bfor\\b\", \"4\", text, flags=re.IGNORECASE)\n    return text\n\n# Step 4: Process capitalization and handle \"edge\" replacement\ndef process_capitalization(text):\n    matches = re.findall(r\"(capital|small) (\\w+)\", text, re.IGNORECASE)\n    cleaned_text = \"\"\n\n    i = 0\n    while i < len(text):\n        match_found = False\n        \n        for match in matches:\n            marker, letter = match\n            marker_index = text.lower().find(f\"{marker.lower()} {letter.lower()}\")\n            \n            if marker_index == i:\n                if marker.lower() == \"capital\":\n                    cleaned_text += letter.upper()\n                else:\n                    cleaned_text += letter.lower()\n                i += len(marker) + 2\n                match_found = True\n                break\n\n        if not match_found:\n            cleaned_text += text[i]\n            i += 1\n\n    # Handle \"edge\" replacement based on the previous marker\n    cleaned_text = re.sub(r\"\\bsmall\\s+edge\\b\", \"h\", cleaned_text, flags=re.IGNORECASE)\n    cleaned_text = re.sub(r\"\\bcapital\\s+edge\\b\", \"H\", cleaned_text, flags=re.IGNORECASE)\n\n    # Remove remaining \"capital\" or \"small\" words\n    cleaned_text = re.sub(r\"\\b(capital|small)\\b\", \"\", cleaned_text, flags=re.IGNORECASE)\n    cleaned_text = re.sub(r\"[^a-zA-Z0-9]\", \"\", cleaned_text)  # Keep only A-Z, a-z, 0-9\n    return cleaned_text\n\n# Function to get Whisper transcription\ndef get_whisper_transcription(audio_file):\n    text = \"\"\n    \n    # Try Whisper transcription up to 3 times if length is < 6\n    for _ in range(3):\n        result = whisper_model.transcribe(audio_file)\n        text = result[\"text\"]\n        text = replace_number_words(text)  # Replace number words with digits\n        text = fix_misheard_words(text)  # Fix misheard words\n        text = replace_for_with_4(text)  # Replace \"for\" with \"4\"\n        text = process_capitalization(text)  # Process capitalization & \"edge\"\n        \n        if len(text) >= 6:  # Ensure minimum length of 6\n            break\n\n    return text[:6]  # Ensure the output is exactly 6 characters\n\ndef get_image_text(image_file):\n    ocr_result = ocr_reader.readtext(image_file)\n    text = ''.join([res[1] for res in ocr_result])\n    return text\n\n# Main function to run the process for random 10 files\ndef process_random_samples(audio_dir, image_dir, num_samples=10):\n    audio_files = random.sample(os.listdir(audio_dir), num_samples)\n    \n    for audio_file in audio_files:\n        audio_path = os.path.join(audio_dir, audio_file)\n        image_file = audio_file.replace('.wav', '.png')  # Assuming same filename for audio and image\n        image_path = os.path.join(image_dir, image_file)\n\n        whisper_text = get_whisper_transcription(audio_path)\n\n        ground_truth_text = get_image_text(image_path)\n\n        print(f\"Audio file: {audio_file}\")\n        print(f\"Whisper AI Transcription: {whisper_text}\")\n        print(f\"Ground Truth (OCR): {ground_truth_text}\")\n        print(\"=\"*50)\n\nprocess_random_samples(audio_dir, image_dir, num_samples=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T12:44:11.712362Z","iopub.execute_input":"2025-02-17T12:44:11.712762Z","iopub.status.idle":"2025-02-17T12:44:38.560011Z","shell.execute_reply.started":"2025-02-17T12:44:11.712732Z","shell.execute_reply":"2025-02-17T12:44:38.559202Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(fp, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Audio file: captcha_4175.wav\nWhisper AI Transcription: mPChvc\nGround Truth (OCR): mPChdc\n==================================================\nAudio file: captcha_2383.wav\nWhisper AI Transcription: 7X8Fqb\nGround Truth (OCR): 7XBFqb\n==================================================\nAudio file: captcha_0550.wav\nWhisper AI Transcription: FZQcBF\nGround Truth (OCR): FZQcB4\n==================================================\nAudio file: captcha_7525.wav\nWhisper AI Transcription: BYVTJD\nGround Truth (OCR): BYVTJD\n==================================================\nAudio file: captcha_0435.wav\nWhisper AI Transcription: zZWUWQ\nGround Truth (OCR): ZZWUWQ\n==================================================\nAudio file: captcha_6497.wav\nWhisper AI Transcription: 2BZ51J\nGround Truth (OCR): 2BZ51J\n==================================================\nAudio file: captcha_2963.wav\nWhisper AI Transcription: w9dKlX\nGround Truth (OCR): wgdKIX\n==================================================\nAudio file: captcha_6342.wav\nWhisper AI Transcription: RjlwWT\nGround Truth (OCR): RjlwWT\n==================================================\nAudio file: captcha_4006.wav\nWhisper AI Transcription: JQmbaG\nGround Truth (OCR): JQmbag\n==================================================\nAudio file: captcha_3294.wav\nWhisper AI Transcription: 5yhjHF\nGround Truth (OCR): SyhjHF\n==================================================\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Whisper AI(medium) Matching Accuracy Scores","metadata":{}},{"cell_type":"code","source":"audio_dir = '/kaggle/input/captcha-dataset/captchas/audio'\nimage_dir = '/kaggle/input/captcha-dataset/captchas/images'\n\nocr_reader = Reader(['en'])  # EasyOCR setup\nwhisper_model = whisper.load_model(\"medium\")  # Whisper model\n\n# Dictionary to convert number words to digits\nnumber_map = {\n    \"zero\": \"0\", \"one\": \"1\", \"two\": \"2\", \"three\": \"3\", \"four\": \"4\",\n    \"five\": \"5\", \"six\": \"6\", \"seven\": \"7\", \"eight\": \"8\", \"nine\": \"9\"\n}\n\n# Dictionary for common misheard words\nmisheard_map = {\n    \"mall\": \"small\", \"moll\": \"small\", \"capitun\": \"capital\", \"capitan\": \"capital\",\n    \"apital\": \"capital\", \"capitole\": \"capital\", \"zimro\": \"0\", \"smaller\": \"a\"\n}\n\n# Step 1: Replace number words with digits\ndef replace_number_words(text):\n    for word, digit in number_map.items():\n        text = re.sub(rf\"\\b{word}\\b\", digit, text, flags=re.IGNORECASE)\n    return text\n\n# Step 2: Fix common misheard words\ndef fix_misheard_words(text):\n    for wrong, correct in misheard_map.items():\n        text = re.sub(rf\"\\b{wrong}\\b\", correct, text, flags=re.IGNORECASE)\n    return text\n\n# Step 3: Replace \"for\" with \"4\"\ndef replace_for_with_4(text):\n    text = re.sub(r\"\\bfor\\b\", \"4\", text, flags=re.IGNORECASE)\n    return text\n\n# Step 4: Process capitalization and handle \"edge\" replacement\ndef process_capitalization(text):\n    matches = re.findall(r\"(capital|small) (\\w+)\", text, re.IGNORECASE)\n    cleaned_text = \"\"\n\n    i = 0\n    while i < len(text):\n        match_found = False\n        \n        for match in matches:\n            marker, letter = match\n            marker_index = text.lower().find(f\"{marker.lower()} {letter.lower()}\")\n            \n            if marker_index == i:\n                if marker.lower() == \"capital\":\n                    cleaned_text += letter.upper()\n                else:\n                    cleaned_text += letter.lower()\n                i += len(marker) + 2\n                match_found = True\n                break\n\n        if not match_found:\n            cleaned_text += text[i]\n            i += 1\n\n    # Handle \"edge\" replacement based on the previous marker\n    cleaned_text = re.sub(r\"\\bsmall\\s+edge\\b\", \"h\", cleaned_text, flags=re.IGNORECASE)\n    cleaned_text = re.sub(r\"\\bcapital\\s+edge\\b\", \"H\", cleaned_text, flags=re.IGNORECASE)\n\n    # Remove remaining \"capital\" or \"small\" words\n    cleaned_text = re.sub(r\"\\b(capital|small)\\b\", \"\", cleaned_text, flags=re.IGNORECASE)\n    cleaned_text = re.sub(r\"[^a-zA-Z0-9]\", \"\", cleaned_text)  # Keep only A-Z, a-z, 0-9\n    return cleaned_text\n\n# Function to get Whisper transcription\ndef get_whisper_transcription(audio_file):\n    text = \"\"\n    \n    # Try Whisper transcription up to 3 times if length is < 6\n    for _ in range(3):\n        result = whisper_model.transcribe(audio_file)\n        text = result[\"text\"]\n        text = replace_number_words(text)  # Replace number words with digits\n        text = fix_misheard_words(text)  # Fix misheard words\n        text = replace_for_with_4(text)  # Replace \"for\" with \"4\"\n        text = process_capitalization(text)  # Process capitalization & \"edge\"\n        \n        if len(text) >= 6:  # Ensure minimum length of 6\n            break\n\n    return text[:6]  # Ensure the output is exactly 6 characters\n\ndef get_image_text(image_file):\n    ocr_result = ocr_reader.readtext(image_file)\n    text = ''.join([res[1] for res in ocr_result])\n    return text\n\ndef process_all_samples(audio_dir, image_dir):\n    all_files = os.listdir(audio_dir)\n    total_files = len(all_files)\n    cer_scores = []\n    \n    for i, audio_file in enumerate(tqdm(all_files, desc=\"Processing\")):\n        audio_path = os.path.join(audio_dir, audio_file)\n        image_file = audio_file.replace('.wav', '.png')  # Assuming matching names\n        image_path = os.path.join(image_dir, image_file)\n\n        whisper_text = get_whisper_transcription(audio_path)\n\n        ground_truth_text = get_image_text(image_path)\n\n        if not ground_truth_text or not whisper_text:\n            continue  \n\n        error_rate = cer(ground_truth_text, whisper_text)  # CER calculation\n        accuracy = max(0, 100 - (error_rate * 100))  # Convert to percentage accuracy\n        cer_scores.append(accuracy)\n\n        # Print progress every 500 samples\n        if (i + 1) % 500 == 0:\n            avg_accuracy = sum(cer_scores) / len(cer_scores)\n            print(f\"\\nProgress: {i+1}/{total_files} samples processed. Current Accuracy: {avg_accuracy:.2f}%\\n\")\n\n    avg_accuracy = sum(cer_scores) / len(cer_scores) if cer_scores else 0\n    print(f\"\\nFinal Accuracy after processing {total_files} samples: {avg_accuracy:.2f}%\\n\")\n\n# Run the function to process all samples\nprocess_all_samples(audio_dir, image_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T10:56:19.670995Z","iopub.execute_input":"2025-02-17T10:56:19.671521Z","iopub.status.idle":"2025-02-17T12:42:41.824197Z","shell.execute_reply.started":"2025-02-17T10:56:19.671480Z","shell.execute_reply":"2025-02-17T12:42:41.822797Z"}},"outputs":[{"name":"stderr","text":"100%|██████████████████████████████████████| 1.42G/1.42G [00:07<00:00, 195MiB/s]\n/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(fp, map_location=device)\nProcessing:   5%|▌         | 500/10000 [10:15<3:02:10,  1.15s/it]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 500/10000 samples processed. Current Accuracy: 79.25%\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  10%|█         | 1000/10000 [20:49<2:52:04,  1.15s/it]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 1000/10000 samples processed. Current Accuracy: 79.66%\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  15%|█▌        | 1500/10000 [31:28<2:47:09,  1.18s/it]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 1500/10000 samples processed. Current Accuracy: 79.42%\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  20%|██        | 2000/10000 [41:45<2:34:18,  1.16s/it]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 2000/10000 samples processed. Current Accuracy: 78.96%\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  25%|██▌       | 2500/10000 [52:14<2:40:49,  1.29s/it]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 2500/10000 samples processed. Current Accuracy: 79.27%\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  30%|███       | 3000/10000 [1:02:28<2:07:28,  1.09s/it]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 3000/10000 samples processed. Current Accuracy: 79.47%\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  35%|███▌      | 3500/10000 [1:12:55<3:18:37,  1.83s/it]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 3500/10000 samples processed. Current Accuracy: 79.47%\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  40%|████      | 4000/10000 [1:23:45<2:03:55,  1.24s/it] ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 4000/10000 samples processed. Current Accuracy: 79.56%\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  45%|████▌     | 4500/10000 [1:34:50<2:37:13,  1.72s/it] ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 4500/10000 samples processed. Current Accuracy: 79.46%\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  50%|█████     | 5000/10000 [1:45:14<1:40:58,  1.21s/it]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 5000/10000 samples processed. Current Accuracy: 79.53%\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  50%|█████     | 5035/10000 [1:45:56<1:44:27,  1.26s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-74a0ab657100>\u001b[0m in \u001b[0;36m<cell line: 131>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;31m# Run the function to process all samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m \u001b[0mprocess_all_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-6-74a0ab657100>\u001b[0m in \u001b[0;36mprocess_all_samples\u001b[0;34m(audio_dir, image_dir)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# Get transcription from Whisper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mwhisper_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_whisper_transcription\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# Get ground truth text from OCR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-74a0ab657100>\u001b[0m in \u001b[0;36mget_whisper_transcription\u001b[0;34m(audio_file)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# Try Whisper transcription up to 3 times if length is < 6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisper_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranscribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplace_number_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace number words with digits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py\u001b[0m in \u001b[0;36mtranscribe\u001b[0;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[0m\n\u001b[1;32m    144\u001b[0m                 )\n\u001b[1;32m    145\u001b[0m             \u001b[0mmel_segment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_or_trim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_FRAMES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_language\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_segment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0mdecode_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"language\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/decoding.py\u001b[0m in \u001b[0;36mdetect_language\u001b[0;34m(model, mel, tokenizer)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# forward pass using a single token, startoftranscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mn_audio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msot\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_audio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [n_audio, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":6},{"cell_type":"markdown","source":"# Whisper AI(medium) - No. of texts matched","metadata":{}},{"cell_type":"code","source":"audio_dir = '/kaggle/input/captcha-dataset/captchas/audio'\nimage_dir = '/kaggle/input/captcha-dataset/captchas/images'\n\nocr_reader = Reader(['en'])  # EasyOCR setup\nwhisper_model = whisper.load_model(\"medium\")  # Whisper model\n\n# Dictionary to convert number words to digits\nnumber_map = {\n    \"zero\": \"0\", \"one\": \"1\", \"two\": \"2\", \"three\": \"3\", \"four\": \"4\",\n    \"five\": \"5\", \"six\": \"6\", \"seven\": \"7\", \"eight\": \"8\", \"nine\": \"9\"\n}\n\n# Dictionary for common misheard words\nmisheard_map = {\n    \"mall\": \"small\", \"moll\": \"small\", \"capitun\": \"capital\", \"capitan\": \"capital\",\n    \"apital\": \"capital\", \"capitole\": \"capital\", \"zimro\": \"0\", \"smaller\": \"a\"\n}\n\n# Step 1: Replace number words with digits\ndef replace_number_words(text):\n    for word, digit in number_map.items():\n        text = re.sub(rf\"\\b{word}\\b\", digit, text, flags=re.IGNORECASE)\n    return text\n\n# Step 2: Fix common misheard words\ndef fix_misheard_words(text):\n    for wrong, correct in misheard_map.items():\n        text = re.sub(rf\"\\b{wrong}\\b\", correct, text, flags=re.IGNORECASE)\n    return text\n\n# Step 3: Replace \"for\" with \"4\"\ndef replace_for_with_4(text):\n    text = re.sub(r\"\\bfor\\b\", \"4\", text, flags=re.IGNORECASE)\n    return text\n\n# Step 4: Process capitalization and handle \"edge\" replacement\ndef process_capitalization(text):\n    matches = re.findall(r\"(capital|small) (\\w+)\", text, re.IGNORECASE)\n    cleaned_text = \"\"\n\n    i = 0\n    while i < len(text):\n        match_found = False\n        \n        for match in matches:\n            marker, letter = match\n            marker_index = text.lower().find(f\"{marker.lower()} {letter.lower()}\")\n            \n            if marker_index == i:\n                if marker.lower() == \"capital\":\n                    cleaned_text += letter.upper()\n                else:\n                    cleaned_text += letter.lower()\n                i += len(marker) + 2\n                match_found = True\n                break\n\n        if not match_found:\n            cleaned_text += text[i]\n            i += 1\n\n    # Handle \"edge\" replacement based on the previous marker\n    cleaned_text = re.sub(r\"\\bsmall\\s+edge\\b\", \"h\", cleaned_text, flags=re.IGNORECASE)\n    cleaned_text = re.sub(r\"\\bcapital\\s+edge\\b\", \"H\", cleaned_text, flags=re.IGNORECASE)\n\n    # Remove remaining \"capital\" or \"small\" words\n    cleaned_text = re.sub(r\"\\b(capital|small)\\b\", \"\", cleaned_text, flags=re.IGNORECASE)\n    cleaned_text = re.sub(r\"[^a-zA-Z0-9]\", \"\", cleaned_text)  # Keep only A-Z, a-z, 0-9\n    return cleaned_text\n\n# Function to get Whisper transcription\ndef get_whisper_transcription(audio_file):\n    text = \"\"\n    \n    # Try Whisper transcription up to 3 times if length is < 6\n    for _ in range(3):\n        result = whisper_model.transcribe(audio_file)\n        text = result[\"text\"]\n        text = replace_number_words(text)  # Replace number words with digits\n        text = fix_misheard_words(text)  # Fix misheard words\n        text = replace_for_with_4(text)  # Replace \"for\" with \"4\"\n        text = process_capitalization(text)  # Process capitalization & \"edge\"\n        \n        if len(text) >= 6:  # Ensure minimum length of 6\n            break\n\n    return text[:6]  # Ensure the output is exactly 6 characters\n\ndef get_image_text(image_file):\n    ocr_result = ocr_reader.readtext(image_file)\n    text = ''.join([res[1] for res in ocr_result])\n    return text\n\ndef process_all_samples(audio_dir, image_dir):\n    all_files = os.listdir(audio_dir)\n    total_files = len(all_files)\n    exact_match_count = 0  # Counter for exact matches\n    \n    for i, audio_file in enumerate(tqdm(all_files, desc=\"Processing\")):\n        audio_path = os.path.join(audio_dir, audio_file)\n        image_file = audio_file.replace('.wav', '.png')  # Assuming matching names\n        image_path = os.path.join(image_dir, image_file)\n\n        whisper_text = get_whisper_transcription(audio_path)\n\n        ground_truth_text = get_image_text(image_path)\n\n        if not ground_truth_text or not whisper_text:\n            continue  \n\n        if whisper_text == ground_truth_text:\n            exact_match_count += 1\n\n        if (i + 1) % 500 == 0:\n            print(f\"\\nProgress: {i+1}/{total_files} samples processed. Exact Matches: {exact_match_count}\\n\")\n\n    # Final count of exact matches\n    print(f\"\\nFinal Exact Matches after processing {total_files} samples: {exact_match_count}\\n\")\n\n# Run the function to process all samples\nprocess_all_samples(audio_dir, image_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T12:46:51.376764Z","iopub.execute_input":"2025-02-17T12:46:51.377062Z","iopub.status.idle":"2025-02-17T16:15:35.556792Z","shell.execute_reply.started":"2025-02-17T12:46:51.377041Z","shell.execute_reply":"2025-02-17T16:15:35.556002Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(fp, map_location=device)\nProcessing:   5%|▌         | 500/10000 [10:07<2:57:02,  1.12s/it]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 500/10000 samples processed. Exact Matches: 151\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  10%|█         | 1000/10000 [20:31<2:51:11,  1.14s/it]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 1000/10000 samples processed. Exact Matches: 296\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  15%|█▌        | 1500/10000 [31:01<2:45:27,  1.17s/it]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 1500/10000 samples processed. Exact Matches: 442\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  20%|██        | 2000/10000 [41:03<2:29:49,  1.12s/it]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 2000/10000 samples processed. Exact Matches: 569\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  25%|██▌       | 2500/10000 [51:25<2:38:26,  1.27s/it]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 2500/10000 samples processed. Exact Matches: 728\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  30%|███       | 3000/10000 [1:01:34<2:05:21,  1.07s/it]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 3000/10000 samples processed. Exact Matches: 885\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  35%|███▌      | 3500/10000 [1:11:52<3:17:36,  1.82s/it]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 3500/10000 samples processed. Exact Matches: 1046\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  40%|████      | 4000/10000 [1:22:21<2:02:06,  1.22s/it]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 4000/10000 samples processed. Exact Matches: 1199\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  45%|████▌     | 4500/10000 [1:33:16<2:34:40,  1.69s/it] ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 4500/10000 samples processed. Exact Matches: 1337\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  50%|█████     | 5000/10000 [1:43:28<1:38:10,  1.18s/it]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 5000/10000 samples processed. Exact Matches: 1492\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  55%|█████▌    | 5500/10000 [1:53:57<1:23:35,  1.11s/it]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 5500/10000 samples processed. Exact Matches: 1637\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  60%|██████    | 6000/10000 [2:04:27<1:18:03,  1.17s/it]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 6000/10000 samples processed. Exact Matches: 1775\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  65%|██████▌   | 6500/10000 [2:14:58<1:08:40,  1.18s/it]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 6500/10000 samples processed. Exact Matches: 1916\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  70%|███████   | 7000/10000 [2:25:28<1:00:21,  1.21s/it]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 7000/10000 samples processed. Exact Matches: 2065\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  75%|███████▌  | 7500/10000 [2:35:52<47:58,  1.15s/it]  ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 7500/10000 samples processed. Exact Matches: 2220\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  80%|████████  | 8000/10000 [2:46:21<40:47,  1.22s/it]  ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 8000/10000 samples processed. Exact Matches: 2362\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  85%|████████▌ | 8500/10000 [2:56:50<28:41,  1.15s/it]  ","output_type":"stream"},{"name":"stdout","text":"\nProgress: 8500/10000 samples processed. Exact Matches: 2510\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  90%|█████████ | 9000/10000 [3:07:45<25:58,  1.56s/it]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 9000/10000 samples processed. Exact Matches: 2668\n\n","output_type":"stream"},{"name":"stderr","text":"Processing:  95%|█████████▌| 9500/10000 [3:18:12<09:45,  1.17s/it]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 9500/10000 samples processed. Exact Matches: 2808\n\n","output_type":"stream"},{"name":"stderr","text":"Processing: 100%|██████████| 10000/10000 [3:28:28<00:00,  1.25s/it]","output_type":"stream"},{"name":"stdout","text":"\nProgress: 10000/10000 samples processed. Exact Matches: 2962\n\n\nFinal Exact Matches after processing 10000 samples: 2962\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10}]}